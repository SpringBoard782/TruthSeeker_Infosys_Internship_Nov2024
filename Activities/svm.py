# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uqjyN17DmrwSyEyiTYBS3uG9bjyIIRt4
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
fake_path = '/content/drive/MyDrive/lemmitized dataset/LemmatizedFake.csv'
true_path = '/content/drive/MyDrive/lemmitized dataset/LemmatizedTrue.csv'
fake_df = pd.read_csv(fake_path)
true_df = pd.read_csv(true_path)
print("LemmatizedFake Dataset:")
print(fake_df.head(3))
print("\nLemmatizedTrue Dataset:")
print(true_df.head(3))

total_rows = fake_df.shape[0] + true_df.shape[0]
print(f"Total rows across both datasets: {total_rows}")

"""**POS**"""

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng') # Download the required data package

!pip install spacy
import spacy

# Download the English language model if you haven't already
!python -m spacy download en_core_web_sm

fake_df_sample = fake_df.sample(1000)  # Use 1,000 rows
true_df_sample = true_df.sample(1000)

fake_df_sample['pos_tags'] = spacy_pos_tagging_batch(fake_df_sample['text'])
true_df_sample['pos_tags'] = spacy_pos_tagging_batch(true_df_sample['text'])

print("Fake Dataset Sample with POS Tags:")
print(fake_df_sample[['text', 'pos_tags']].head())

print("\nTrue Dataset Sample with POS Tags:")
print(true_df_sample[['text', 'pos_tags']].head())

fake_df_sample.to_csv('/content/processed_fake_sample.csv', index=False)
true_df_sample.to_csv('/content/processed_true_sample.csv', index=False)

from collections import Counter

def pos_frequency(pos_tags):
    # Modified to handle potential variations in pos_tags structure
    pos_list = []
    for item in pos_tags:  # Iterate through elements of pos_tags
        if isinstance(item, tuple) and len(item) >= 2:  # Check if item is a tuple with at least 2 elements
            pos_list.append(item[1])  # Append the second element (POS tag)
        elif isinstance(item, str):  # Check if item is a string (possibly just the POS tag)
            pos_list.append(item)
    return Counter(pos_list)  # Return the frequency count

fake_sample_pos_freq = fake_df_sample['pos_tags'].apply(pos_frequency)
true_sample_pos_freq = true_df_sample['pos_tags'].apply(pos_frequency)

print("POS Frequency in Fake Dataset Sample:")
print(fake_sample_pos_freq.head())

print("\nPOS Frequency in True Dataset Sample:")
print(true_sample_pos_freq.head())



from sklearn.feature_extraction import DictVectorizer

# Convert the 'pos_tags' to dictionaries for vectorization
# Extract only the token and POS tag from each tuple
combined_sample['pos_features'] = combined_sample['pos_tags'].apply(lambda tags: {tag[0]: tag[1] for tag in tags if isinstance(tag, tuple) and len(tag) >= 2})

# Vectorize the POS frequency features
vectorizer = DictVectorizer(sparse=False)  # Use sparse=True for large datasets
X = vectorizer.fit_transform(combined_sample['pos_features'])
y = combined_sample['label']

from sklearn.model_selection import train_test_split

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the size of each set
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")

print(combined_sample['pos_tags'].head())

print(combined_sample['pos_tags'].tail())

"""**NER**"""

import spacy

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Disable components not needed for NER
with nlp.disable_pipes('ner', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer'):
    fake_df_sample['ner_entities'] = fake_df_sample['text'].apply(lambda text: spacy_ner(text))
    true_df_sample['ner_entities'] = true_df_sample['text'].apply(lambda text: spacy_ner(text))

# Print the results to verify
print("Fake Dataset Sample with NER Entities:")
print(fake_df_sample[['text', 'ner_entities']].head())

print("\nTrue Dataset Sample with NER Entities:")
print(true_df_sample[['text', 'ner_entities']].head())

# Count the occurrences of different entity types
fake_entities = fake_df_sample['ner_entities'].explode().value_counts()
true_entities = true_df_sample['ner_entities'].explode().value_counts()

print("Fake Dataset Entities Frequency:")
print(fake_entities)

print("\nTrue Dataset Entities Frequency:")
print(true_entities)

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
fake_entity_features = mlb.fit_transform(fake_df_sample['ner_entities'])
true_entity_features = mlb.transform(true_df_sample['ner_entities'])
X_fake = pd.DataFrame(index=fake_df_sample.index)
X_true = pd.DataFrame(index=true_df_sample.index)
X_fake = pd.concat([X_fake, pd.DataFrame(fake_entity_features, columns=mlb.classes_, index=fake_df_sample.index)], axis=1)
X_true = pd.concat([X_true, pd.DataFrame(true_entity_features, columns=mlb.classes_, index=true_df_sample.index)], axis=1)

existing_features_fake = fake_df_sample[['title', 'text']]
existing_features_true = true_df_sample[['title', 'text']]

X_fake = pd.concat([X_fake, existing_features_fake], axis=1)
X_true = pd.concat([X_true, existing_features_true], axis=1)

existing_features_fake = fake_df_sample[['title', 'text']]
existing_features_true = true_df_sample[['title', 'text']]

X_fake = pd.concat([X_fake, existing_features_fake], axis=1)
X_true = pd.concat([X_true, existing_features_true], axis=1)

y_fake = fake_df_sample['label']
y_true = true_df_sample['label']

X_combined = pd.concat([X_fake, X_true], axis=0)
y_combined = pd.concat([y_fake, y_true], axis=0)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # You can adjust the number of features
X_fake_text = vectorizer.fit_transform(fake_df_sample['text'])
X_true_text = vectorizer.transform(true_df_sample['text'])
X_fake_text_df = pd.DataFrame(X_fake_text.toarray(), columns=vectorizer.get_feature_names_out(), index=fake_df_sample.index)
X_true_text_df = pd.DataFrame(X_true_text.toarray(), columns=vectorizer.get_feature_names_out(), index=true_df_sample.index)
X_fake = pd.concat([X_fake, X_fake_text_df], axis=1)
X_true = pd.concat([X_true, X_true_text_df], axis=1)
X_combined = pd.concat([X_fake, X_true], axis=0)
y_combined = pd.concat([y_fake, y_true], axis=0)

X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)
print(X_train.head())

"""**TF**-**IDF**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pandas as pd
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
all_text = pd.concat([fake_df_sample['text'], true_df_sample['text']])
vectorizer.fit(all_text)
X_fake_text = vectorizer.transform(fake_df_sample['text'])
X_true_text = vectorizer.transform(true_df_sample['text'])
X_fake_text_df = pd.DataFrame(X_fake_text.toarray(), columns=vectorizer.get_feature_names_out(), index=fake_df_sample.index)
X_true_text_df = pd.DataFrame(X_true_text.toarray(), columns=vectorizer.get_feature_names_out(), index=true_df_sample.index)
X_fake = X_fake.drop(columns=['text', 'title'], errors='ignore')
X_true = X_true.drop(columns=['text', 'title'], errors='ignore')
X_fake = pd.concat([X_fake, X_fake_text_df], axis=1)
X_true = pd.concat([X_true, X_true_text_df], axis=1)
X_combined = pd.concat([X_fake, X_true], axis=0)
y_combined = pd.concat([y_fake, y_true], axis=0)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=3)
grid_search.fit(X_train, y_train)
print(grid_search.best_params_)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(clf, X_combined, y_combined, cv=5)
print("Cross-validation scores:", scores)

import joblib
joblib.dump(clf, 'random_forest_model.pkl')

import joblib

# Save the model and vectorizer
joblib.dump(clf, 'random_forest_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

import joblib
model = joblib.load('random_forest_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')
new_data = ["Example text to classify"]
feature_names = vectorizer.get_feature_names_out()
new_data_transformed = vectorizer.transform(new_data)
new_data_df = pd.DataFrame(new_data_transformed.toarray(), columns=feature_names)
model_features = model.feature_names_in_
missing_features = set(model_features) - set(new_data_df.columns)
for feature in missing_features:
    new_data_df[feature] = 0
new_data_df = new_data_df[model_features]
predictions = model.predict(new_data_df)
print("Predictions:", predictions)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""NB **Classifier**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix

nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

y_pred_nb = nb_classifier.predict(X_test)

accuracy_nb = nb_classifier.score(X_test, y_test)
print(f"Accuracy: {accuracy_nb}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_nb))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_nb))

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)
y_pred_nb = nb_classifier.predict(X_test)
accuracy_nb = nb_classifier.score(X_test, y_test)
print(f"Accuracy: {accuracy_nb}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_nb))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_nb))

import joblib
joblib.dump(nb_classifier, 'naive_bayes_model.pkl')

"""**SVM**"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train, y_train)

y_pred_svm = svm_classifier.predict(X_test)

accuracy_svm = svm_classifier.score(X_test, y_test)
print(f"Accuracy: {accuracy_svm}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

""" Tuning **Hyperparameters**"""

param_grid = {
    'C': [0.1, 1],  # Reduce the range of C
    'kernel': ['linear'],  # Test with only one kernel
    'gamma': ['scale']  # Use only one gamma value
}

X_train_subset, _, y_train_subset, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)
grid_search.fit(X_train_subset, y_train_subset)

!pip install scipy
from scipy.stats import uniform

param_dist = {
    'C': uniform(0.1, 2),
    'kernel': ['linear'],
    'gamma': ['scale']
}

from sklearn.model_selection import train_test_split
import numpy as np

X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([0, 1, 0])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.svm import SVC
from scipy.stats import uniform
import numpy as np
X = np.array([[1, 2], [3, 4], [5, 6]])
y = np.array([0, 1, 0])
random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=5, cv=2, scoring='accuracy', random_state=42)
random_search.fit(X, y)

param_dist = {
    'C': uniform(0.1, 10),  # Uniform distribution for 'C' between 0.1 and 10
    'kernel': ['linear', 'rbf'],  # Possible kernel types
    'gamma': ['scale', 'auto']  # Options for gamma
}

random_search.fit(X, y)

print("Best Parameters:", random_search.best_params_)

best_svm = random_search.best_estimator_
print("Best Model:", best_svm)

from sklearn.metrics import classification_report, confusion_matrix
y_pred = best_svm.predict(X)
print("Classification Report:")
print(classification_report(y, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y, y_pred))