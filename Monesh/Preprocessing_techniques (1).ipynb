{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fxoasmA4yVch"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"\n",
        "Hello Welcome, to session of NLP Tutorials. We will perform preprocessing.\n",
        "I can't believe this happenin! NLP is, like, super exciting @times & we don't even know why. Ism't it?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9nfc4HRytcR",
        "outputId": "3179a5cd-abd6-49b2-c9ba-4a269458d23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello Welcome, to session of NLP Tutorials. We will perform preprocessing.\n",
            "I can't believe this happenin! NLP is, like, super exciting @times & we don't even know why. Ism't it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZmts3nuzLjk"
      },
      "source": [
        "## Text Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tA4eJFDCzIoz"
      },
      "outputs": [],
      "source": [
        "corpus=corpus.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyA8sdvrzaII",
        "outputId": "708b0c61-b0ec-495a-adb2-1d082041ce8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hello welcome, to session of nlp tutorials. we will perform preprocessing.\n",
            "i can't believe this happenin! nlp is, like, super exciting @times & we don't even know why. ism't it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xUW1AnDkzdTk"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZOF0_IS1zxSH"
      },
      "outputs": [],
      "source": [
        "text=re.sub(r'[^\\w\\s]',\"\",corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoUvOuLj0Uub",
        "outputId": "a796f145-6b14-42b5-ea69-0527bb244f74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hello welcome to session of nlp tutorials we will perform preprocessing\n",
            "i cant believe this happenin nlp is like super exciting times  we dont even know why ismt it\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VZwcTJJ40V_E"
      },
      "outputs": [],
      "source": [
        "## Removing stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kNo-tgb51bRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcd1cc6-440f-4c1a-a131-db968a56ef19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW8I80sl1fV1",
        "outputId": "34800ab1-a339-4e5f-dd6a-435ab07b8cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QLgJpiMn2CgE"
      },
      "outputs": [],
      "source": [
        "stop_words=set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kUVgIht72Tgr",
        "outputId": "6a713712-8527-465c-8999-bef6d39fd78e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GULq09xc1wYJ"
      },
      "outputs": [],
      "source": [
        "text=\"\".join([word for word in text.split() if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VHzqx_TU2P87",
        "outputId": "4acda4a0-5105-4c2a-f3ca-fbebd804268f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hellowelcomesessionnlptutorialsperformpreprocessingcantbelievehappeninnlplikesuperexcitingtimesdontevenknowismt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_EAYtKL2mpn",
        "outputId": "24b3687e-13f5-4797-a94c-188e04bd270d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W6BI9fSj2QeX"
      },
      "outputs": [],
      "source": [
        "from contractions import contractions_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nMs-7E7V2o8i"
      },
      "outputs": [],
      "source": [
        "from contractions import contractions_dict\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwA3d1S32zXJ",
        "outputId": "d9e4d822-dfe1-4280-e475-d9e7ab7f1ec3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"hellowelcomesessionnlptutorialsperformpreprocessingcantbelievehappeninnlplikesuperexcitingtimesdontevenknowismt\")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "TextBlob(text).correct()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkM8frh539Zb"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OZZ-ZWwi4Ei3"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys1FoUlE5UQk",
        "outputId": "d954283d-ad9b-49bb-fec0-3202d6f538b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66rCIoz64_2M",
        "outputId": "d1a8b4f1-3ec8-445a-de1a-6ac13a0bb90a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nhello welcome, to session of nlp tutorials.',\n",
              " 'we will perform preprocessing.',\n",
              " \"i can't believe this happenin!\",\n",
              " \"nlp is, like, super exciting @times & we don't even know why.\",\n",
              " \"ism't it?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DVT05xe15C7r"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGYxZGc85oXA",
        "outputId": "d9391982-0bf4-45f2-ac6e-4d47d9d596ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'session',\n",
              " 'of',\n",
              " 'nlp',\n",
              " 'tutorials',\n",
              " '.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'perform',\n",
              " 'preprocessing',\n",
              " '.',\n",
              " 'i',\n",
              " 'ca',\n",
              " \"n't\",\n",
              " 'believe',\n",
              " 'this',\n",
              " 'happenin',\n",
              " '!',\n",
              " 'nlp',\n",
              " 'is',\n",
              " ',',\n",
              " 'like',\n",
              " ',',\n",
              " 'super',\n",
              " 'exciting',\n",
              " '@',\n",
              " 'times',\n",
              " '&',\n",
              " 'we',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'even',\n",
              " 'know',\n",
              " 'why',\n",
              " '.',\n",
              " \"ism't\",\n",
              " 'it',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8zB7oO54HV"
      },
      "source": [
        "Word Punctuation token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oSYjCWTu6DoO"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aic2PPp6xGs",
        "outputId": "6da91474-421f-4986-863e-a1bf39afd3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "hello welcome, to session of nlp tutorials. we will perform preprocessing.\n",
            "i can't believe this happenin! nlp is, like, super exciting @times & we don't even know why. ism't it?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJNYZ2g96Hb1",
        "outputId": "d3a0059a-9992-45ae-bdc9-ec430cc3ca92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'session',\n",
              " 'of',\n",
              " 'nlp',\n",
              " 'tutorials',\n",
              " '.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'perform',\n",
              " 'preprocessing',\n",
              " '.',\n",
              " 'i',\n",
              " 'can',\n",
              " \"'\",\n",
              " 't',\n",
              " 'believe',\n",
              " 'this',\n",
              " 'happenin',\n",
              " '!',\n",
              " 'nlp',\n",
              " 'is',\n",
              " ',',\n",
              " 'like',\n",
              " ',',\n",
              " 'super',\n",
              " 'exciting',\n",
              " '@',\n",
              " 'times',\n",
              " '&',\n",
              " 'we',\n",
              " 'don',\n",
              " \"'\",\n",
              " 't',\n",
              " 'even',\n",
              " 'know',\n",
              " 'why',\n",
              " '.',\n",
              " 'ism',\n",
              " \"'\",\n",
              " 't',\n",
              " 'it',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nrveDY6z6sil"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer=TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EDF5D-u86Tc",
        "outputId": "9ba1bb54-3286-4e58-cbf5-92b98e744b66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'session',\n",
              " 'of',\n",
              " 'nlp',\n",
              " 'tutorials.',\n",
              " 'we',\n",
              " 'will',\n",
              " 'perform',\n",
              " 'preprocessing.',\n",
              " 'i',\n",
              " 'ca',\n",
              " \"n't\",\n",
              " 'believe',\n",
              " 'this',\n",
              " 'happenin',\n",
              " '!',\n",
              " 'nlp',\n",
              " 'is',\n",
              " ',',\n",
              " 'like',\n",
              " ',',\n",
              " 'super',\n",
              " 'exciting',\n",
              " '@',\n",
              " 'times',\n",
              " '&',\n",
              " 'we',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'even',\n",
              " 'know',\n",
              " 'why.',\n",
              " \"ism't\",\n",
              " 'it',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05a2b046",
        "outputId": "6cb56ce2-7cc0-4092-f051-377c4706c971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Text:\n",
            "hellowelcomesessionnlptutorialsperformpreprocessingcantbelievehappeninnlplikesuperexcitingtimesdontevenknowismt\n",
            "Lemmatized Text:\n",
            "hellowelcomesessionnlptutorialsperformpreprocessingcantbelievehappeninnlplikesuperexcitingtimesdontevenknowismt\n"
          ]
        }
      ],
      "source": [
        "# Stemming and Lemmatization\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Applying stemming\n",
        "stemmed_text = \" \".join([stemmer.stem(word) for word in word_tokenize(text)])\n",
        "print(\"Stemmed Text:\")\n",
        "print(stemmed_text)\n",
        "\n",
        "# Applying lemmatization\n",
        "lemmatized_text = \" \".join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
        "print(\"Lemmatized Text:\")\n",
        "print(lemmatized_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}